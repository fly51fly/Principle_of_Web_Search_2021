{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "70c90d70-b101-4444-b8ac-09e8749dd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import bisect\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "import pickle\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict    \n",
    "    \n",
    "class BM25():\n",
    "    def __init__(self, doc_list):\n",
    "        self.doc_count = len(doc_list)\n",
    "        self.avgdl = 0\n",
    "        self.df = defaultdict(int)\n",
    "        for doc in doc_list:\n",
    "            for word in set(jieba.cut(doc)):\n",
    "                self.df[word] += 1\n",
    "            self.avgdl += len(doc)\n",
    "        self.avgdl /= self.doc_count\n",
    "    \n",
    "    def score(self, q, doc):\n",
    "        k1 = 1.5\n",
    "        b = 0.75\n",
    "        result = 0\n",
    "        query_new = set(jieba.cut(q.lower())) - set(['(', ')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ', ''])\n",
    "        word_list_doc = list(jieba.cut(doc.lower()))\n",
    "        for keyword in query_new:\n",
    "            f = word_list_doc.count(keyword)\n",
    "            dl = len(doc)\n",
    "            idf = log((self.doc_count - self.df[keyword] + 0.5) / (self.df[keyword] + 0.5) + 1)\n",
    "            result += idf * ((f * (k1 + 1)) / (f + k1 * (1 - b + b * dl / self.avgdl)))\n",
    "        return result\n",
    "    \n",
    "\n",
    "class MySearchC6V0():\n",
    "    \"\"\"\n",
    "    C3V0: Base class for Search Engine.\n",
    "    C3V1: Data multiplication added.\n",
    "    C3V2: Sorting optimization.\n",
    "    C3V3: Add lowered version of docs.\n",
    "    C3V4: For long doc.\n",
    "    C3V5: Caching search results.\n",
    "    C3V6: Pre-caching all words in docs.\n",
    "    C3V7: Add Serialize/UnSerialize.\n",
    "    C4V1: Add basic Bool query support\n",
    "    C4V2: Add wordseg to get_word_match()\n",
    "    ----------------C5V0-----------------\n",
    "    C5V1: Use VSMTFIDF.score() as score\n",
    "    C5V2: Use BM25.score() as score\n",
    "    C5V3: Use MiddleRank -> BM25\n",
    "    ----------------C6V0-----------------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    filename : str\n",
    "        file name of doc data\n",
    "    multi_factor : int\n",
    "        data multiplication factor(default 1)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    load_data(filename):\n",
    "        load data from file.\n",
    "    save_data(filename):\n",
    "        save data to file\n",
    "    pre_cache_all():\n",
    "        Pre-caching all words in docs.\n",
    "    highlight(text, keyword):\n",
    "        highlight text with keyword.\n",
    "    score(text, keyword):\n",
    "        get score of text for a query.\n",
    "    get_word_match(self, keyword):\n",
    "        get doc set containing keyword.\n",
    "    search(keyword, num=15):\n",
    "        get top num search results of a query.\n",
    "    render(result_list, keyword):\n",
    "        output search results with highlight.\n",
    "    query_to_set_expression(query):\n",
    "        convert bool query to set expression(for eval process).\n",
    "    get_word_match(word):\n",
    "        get match set of the word.\n",
    "    def mid_score(query, tid):\n",
    "        get middle-rank score of doc(tid) according to query\n",
    "    def cosine(vec1, vec2):\n",
    "        get cosine similarity between vec1 and vec2\n",
    "    def dot(vec1, vec2):\n",
    "        get dot product of vec1 and vec2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, multi_factor=1):\n",
    "        self.docs = []\n",
    "        self.docs_lower = []\n",
    "        self.doc_word_dict = [] #记录文档-词关系\n",
    "        self.search_cache = defaultdict(set)\n",
    "        self.multi_factor = multi_factor\n",
    "        self.load_data(filename)\n",
    "    \n",
    "    def highlight(self, text, keyword, ori_text):\n",
    "        idx = text.find(keyword)\n",
    "        result = text\n",
    "        if idx >= 0:\n",
    "            ori_keyword = ori_text[idx:idx+len(keyword)]\n",
    "            result = ori_text.replace(ori_keyword, f'<span style=\"color:red\">{ori_keyword}</span>')\n",
    "        return result\n",
    "    \n",
    "    def score(self, text, keyword):\n",
    "        result = text.count(keyword)\n",
    "        return result\n",
    "    \n",
    "    def query_to_set_expression(self, query):\n",
    "        query_new_parts = []\n",
    "        all_parts = list(query.replace('(', ' ( ').replace(')', ' ) ').split())\n",
    "        idx = 0\n",
    "        cache = ''\n",
    "        count_parts = len(all_parts)\n",
    "        while idx < count_parts:\n",
    "            if all_parts[idx] == '(' or all_parts[idx] == ')':\n",
    "                query_new_parts.append(all_parts[idx])\n",
    "            elif all_parts[idx] == ' ' or all_parts[idx] == '':\n",
    "                query_new_parts.append(' ')\n",
    "            elif all_parts[idx] in ('and', 'AND', '+'):\n",
    "                query_new_parts.append('&')\n",
    "            elif all_parts[idx] in ('or', 'OR'):\n",
    "                query_new_parts.append('|')\n",
    "            elif all_parts[idx] in ('not', 'NOT', '-'):\n",
    "                query_new_parts.append('-')\n",
    "            else:\n",
    "                if cache:\n",
    "                    cache += ' ' + all_parts[idx]\n",
    "                else:\n",
    "                    cache = all_parts[idx]\n",
    "\n",
    "                if (idx + 1 == count_parts\n",
    "                  or all_parts[idx + 1] in ('(', ')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ', '')):\n",
    "                    query_new_parts.append(f\"self.get_word_match('{cache}')\")\n",
    "                    cache = ''\n",
    "            idx += 1\n",
    "        query_new = ''.join(query_new_parts)\n",
    "        return query_new\n",
    "    \n",
    "    def get_word_match(self, word):\n",
    "        if_first_subword = True\n",
    "        result = None\n",
    "        for term in list(jieba.cut(word)):\n",
    "            if if_first_subword:\n",
    "                result = self.search_cache[term]\n",
    "                if_first_subword = False\n",
    "            else:\n",
    "                result = result & self.search_cache[term]\n",
    "            if not result:\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    def search(self, query, num=15):\n",
    "        query_lower = query.lower()    \n",
    "        result_list = []\n",
    "        min_score = 0\n",
    "        #粗筛(候选文档)\n",
    "        query_set_expression = self.query_to_set_expression(query_lower)\n",
    "        match_tid_list = list(eval(query_set_expression))\n",
    "\n",
    "        query_new = ' '.join(set(jieba.cut(query_lower)) - set(['(', ')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ', '']))\n",
    "        \n",
    "        #粗排(快速排序)\n",
    "        mid_tid_list = [(tid, self.mid_score(query_new, tid)) for tid in match_tid_list]\n",
    "        mid_tid_list.sort(key = lambda x: x[1], reverse=True)\n",
    "        \n",
    "        #细排序\n",
    "        bm25_model = BM25([self.docs_lower[tid] for tid,_ in mid_tid_list[:num + 5]])\n",
    "        result_list = [(tid, bm25_model.score(query_new, self.docs_lower[tid])) for tid,_ in mid_tid_list]\n",
    "        result_list.sort(key = lambda x: x[1], reverse=True)\n",
    "                           \n",
    "        return [doc_id for doc_id, _ in result_list[:num]]\n",
    "            \n",
    "    def pre_cache_all(self):\n",
    "        for tid, doc in enumerate(self.docs_lower):\n",
    "            doc_tf_dict = defaultdict(int)\n",
    "            doc_word_count = 0\n",
    "            for word in jieba.cut_for_search(doc):\n",
    "                self.search_cache[word].add(tid)\n",
    "                doc_tf_dict[word] += 1\n",
    "                doc_word_count += 1\n",
    "            for word in doc_tf_dict:\n",
    "                doc_tf_dict[word] /= doc_word_count\n",
    "            self.doc_word_dict.append(doc_tf_dict)\n",
    "    \n",
    "    def render(self, result_list, keyword):\n",
    "        count = 1\n",
    "        for item in result_list:\n",
    "            result = self.highlight(\n",
    "                self.docs_lower[item], \n",
    "                keyword.lower(), \n",
    "                self.docs[item]\n",
    "            ).replace('$$$', '<br/>') #\n",
    "            display(HTML(f\"{count}、{result[:150]}......\")) #\n",
    "            count += 1\n",
    "            \n",
    "    def mid_score(self, query, tid):\n",
    "        vacabulary = list(set(query.split()))\n",
    "        q_vec = [1] * len(vacabulary)\n",
    "        d_vec = [self.doc_word_dict[tid][word] for word in vacabulary]\n",
    "        score = self.cosine(q_vec, d_vec)\n",
    "        return score\n",
    "    \n",
    "    def dot(self, vec1, vec2):\n",
    "        return [vec1[i] * vec2[i] for i in range(len(vec1))]\n",
    "    \n",
    "    def cosine(self, vec1, vec2):\n",
    "        return sum(self.dot(vec1, vec2)) / (sqrt(sum(self.dot(vec1, vec1))) * sqrt(sum(self.dot(vec2, vec2))))\n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        if filename[-3:] == 'txt':\n",
    "            with open(filename, 'r') as f:\n",
    "                self.docs = f.read().split('\\n')\n",
    "            self.docs_lower = [doc.lower() for doc in self.docs]\n",
    "            self.docs = self.docs * self.multi_factor \n",
    "            self.docs_lower = self.docs_lower * self.multi_factor\n",
    "            self.pre_cache_all()\n",
    "        elif filename[-3:] == 'dat':\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.docs, self.docs_lower, self.search_cache, self.doc_word_dict = pickle.load(f)\n",
    "                \n",
    "    def save_data(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump((self.docs, self.docs_lower, self.search_cache, self.doc_word_dict), f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d1bd81d5-bbfc-4530-991e-81596fe9baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = MySearchC6V0('titles_l.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0498a5c3-ffa4-4370-b345-a7e685cb05e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.search_cache['0-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd11d526-d37c-498e-8b40-09e9cc01ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1、德国杯-爆冷!拜仁0-5惨遭门兴血洗 后防灾难级表现<br/>北京时间10月28日凌晨2：45，2021-2022赛季德国杯第二轮继续进行，拜仁慕尼黑客场对阵门兴格拉德巴赫，上半场，科内闪电破门，本塞拜尼连入两球，拜仁<span style=\"color:red\">0-3</span>落后。下半场，......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "2、山东确诊+2、阳性+3！与此前确诊有密接<br/>根据最新通报，初步流调显示，1例确诊病例徐某某和3例阳性检测者耿某某、王某某、陈某某为首例确诊病例赵某某在饭店就餐时的密切接触者。1例确诊病例解某某的家庭成员与首例确诊病例赵某某有时空轨迹交集。更详细的轨迹调查正在流调中。根据此前通报，山东日照25日......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3、我国新增本土病例+1：这个城市要重点注意了！<br/>10月3日，新疆伊犁州霍尔果斯市发现2例新冠肺炎无症状感染者。当日，伊犁州霍尔果斯市已排查密切接触者192人，全部落实隔离医学观察措施。目前按照相关要求，在伊犁州旅游的游客暂时无法离开伊犁，当地正为游客进行安置。针对部分游客逗留在火车站的情况，工......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4、拍1发3！手工枇杷秋梨膏优惠70元：19.9元包邮、0添加剂<br/>秋冬季节，宜进补食疗。天猫奥祥堂食品旗舰店，纯手工枇杷秋梨膏300g×3瓶报价89.9元，限时限量70元券，实付19.9元包邮。精选高品质安徽砀山酥梨为主，还添加了罗汉果、百合、菊花、甘草、玉竹、枇杷为辅，古法匠心熬制十几小时，高......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "5、定了 厦门地铁明日起全面恢复运营：零新增<br/>据@厦门地铁 官方消息，据厦门卫健委通报，10月4日0—24时，厦门市报告新增境外输入新型冠状病毒肺炎确诊病例2例，新增境外输入无症状感染者0例。自9月12日以来，厦门市累计报告本土确诊病例236例，目前住院135例；现有本土无症状感染者尚在接受隔离......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6、哈尔滨确诊+3！幼儿园中小学紧急叫停<br/>据哈尔滨市疫情防控专场新闻发布会通报，详情如下：确诊病例1：新冠肺炎确诊病例（轻型），现住巴彦县兴隆林业局利民家园。确诊病例2：新冠肺炎确诊病例（普通型），现住巴彦县兴隆林业局安民家园。确诊病例3：新冠肺炎确诊病例（普通型），现住巴彦县兴隆林业局安民家园......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "7、全国新增确诊26例：24天来第一次好消息！<br/>据国家卫健委，10月4日0-24时，31个省（自治区、直辖市）和新疆生产建设兵团报告，其中上海6例、云南5例、山东4例、广东4例、陕西3例、福建2例、吉林1例、黑龙江1例。同时，，为境外输入病例，在上海；，均为境外输入；截至10月4日24时，31个......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "8、福建本土新增明显下降：累计报告本土确诊418例！<br/>9月21日0—24时，当日新增治愈出院病例37例，解除医学观察的密切接触者797人，重症病例较前一日增加2例。境外输入现有确诊病例532例（其中重症病例5例），现有疑似病例4例。累计确诊病例8868例，累计治愈出院病例8336例，无死亡病例。......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "9、哈尔滨新增3例确诊病例：情况不容乐观！<br/>病例2、病例3为菲律宾境外返哈解除隔离人员，于8月3日从菲律宾尼诺阿基诺国际机场乘坐5j308航班入境广州白云机场，在广州卡西国际酒店医学隔离14天，8月18日解除集中隔离后，当日乘坐广州jd5165航班抵达哈尔滨太平国际机场，8月18日至9月1日在巴......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "10、2天确诊11例 哈尔滨出来的人去哪了？这动向注意了！<br/>9月1日以来，哈尔滨的人口都流向了哪里？健康时报记者根据百度迁徙地图9月1日至9月21日数据统计发现，截止发稿，哈尔滨市尚未通报此轮疫情的源头。不过，值得注意的是，此次疫情中首例发现的病例1曾有省外旅居史；病例2、3为病例1的密切接触者，......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = '0-3'\n",
    "search_result = searcher.search(query, num=10)\n",
    "searcher.render(search_result, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f4cd00f3-3a01-423c-888d-1064002dc436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11 | - | 29'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' | '.join(jieba.cut_for_search('11-29'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f52d2-d865-48a7-a5f5-9b1882b0dd3e",
   "metadata": {},
   "source": [
    "### 对中英/数混排文档分词粒度的思考   \n",
    "### 解决方案：  \n",
    "#### 对中英文/数字片段分别处理  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dfc85eef-cc5a-4407-9e5c-d49103bf562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def parse_doc(doc):\n",
    "    result = []\n",
    "    state_last = ''\n",
    "    cache = ''\n",
    "    for c in doc:\n",
    "        state_c = c in string.ascii_letters \\\n",
    "            or c.isdigit() \\\n",
    "            or c in ('-', ':', '.', '：', '/')\n",
    "        if c == ' ':\n",
    "            if state_last:\n",
    "                result.append(cache)\n",
    "            else:\n",
    "                result.extend(list(jieba.cut_for_search(cache)))\n",
    "            result.append(' ')\n",
    "            cache = ''\n",
    "            state_last = '' \n",
    "        else:\n",
    "            if state_c == state_last:\n",
    "                cache += c\n",
    "            else:\n",
    "                if state_last != '':\n",
    "                    if state_last:\n",
    "                        result.append(cache)\n",
    "                    else:\n",
    "                        result.extend(list(jieba.cut_for_search(cache)))\n",
    "                cache = c\n",
    "            state_last = state_c\n",
    "    if cache:\n",
    "        if state_last:\n",
    "            result.append(cache)\n",
    "        else:\n",
    "            result.extend(list(jieba.cut_for_search(cache)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "20c83808-334a-440f-91e4-39459d1a9fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'德国 | 德国杯 | - | 爆冷 | ! | 拜仁 | 0-5 | 惨遭 | 门兴 | 血洗 |   | 后防 | 灾难 | 级 | 表现 | $ | $ | $ | 北京 | 时间 | 10 | 月 | 28 | 日 | 凌晨 | 2：45 | ， | 2021-2022 | 赛季 | 德国 | 德国杯 | 第二 | 二轮 | 第二轮 | 继续 | 进行 | ， | 拜仁 | 慕尼黑 | 客场 | 对阵 | 门兴格 | 拉 | 德巴 | 巴赫 | 德巴赫 | ， | 上半 | 半场 | 上半场 | ， | 科内 | 闪电 | 破门 | ， | 本塞拜尼 | 连入 | 两球 | ， | 拜仁 | 0-3 | 落后 | 。 | 下半 | 半场 | 下半场 | ， | 恩博洛 | 二度 | 梅开二度 | 。 | 最终 | ， | 拜仁 | 客场 | 0-5 | 惨败 | 于门兴 | ， | 德国 | 德国杯 | 惨遭 | 淘汰 | ！ | $ | $ | $ | https://www.163.com/sports/article/GNCKODUB00058781.html'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = '''德国杯-爆冷!拜仁0-5惨遭门兴血洗 后防灾难级表现$$$北京时间10月28日凌晨2：45，2021-2022赛季德国杯第二轮继续进行，拜仁慕尼黑客场对阵门兴格拉德巴赫，上半场，科内闪电破门，本塞拜尼连入两球，拜仁0-3落后。下半场，恩博洛梅开二度。最终，拜仁客场0-5惨败于门兴，德国杯惨遭淘汰！$$$https://www.163.com/sports/article/GNCKODUB00058781.html'''\n",
    "' | '.join(parse_doc(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1059f13d-5484-4f34-b689-40decd1c348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c拜仁慕尼黑', 'e0-5', 's ', 'eand', 's ', 'c德国杯']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_char_type(c):\n",
    "    \"\"\"返回当前字符的类型(e,c,s,f,b)\n",
    "\n",
    "    Args:\n",
    "        c:要进行判断的单个字符\n",
    "\n",
    "    Results:\n",
    "        返回判断结果(前缀)：e为英文，c为中文，s为空格，f为引号，b为括号\n",
    "    \"\"\"\n",
    "    result = 'c'\n",
    "    if c in string.ascii_letters \\\n",
    "            or c.isdigit() \\\n",
    "            or c in ('-', ':', '.', '：', '/'):\n",
    "        result = 'e'\n",
    "    elif c == '\"':\n",
    "        result = 'f'\n",
    "    elif c == ' ':\n",
    "        result = 's'\n",
    "    elif c in ('(', ')'):\n",
    "        result = 'b'\n",
    "    return result\n",
    "\n",
    "def parse_query(doc):\n",
    "    \"\"\"对查询进行自定义解析，保留英文串，对中文串原型插入\n",
    "\n",
    "    Args:\n",
    "        doc:待解析的原始文档\n",
    "\n",
    "    Returns:\n",
    "        解析结果列表，元素是带有串类型标记(首字符，e为英文，c为中文，s为空格，f为引号，b为括号)\n",
    "        的切分term结果\n",
    "    \"\"\"\n",
    "    doc = doc.lower() + ' ' #解决末位字符状态切换问题的小技巧\n",
    "    result = []\n",
    "    doclen = len(doc)\n",
    "    i = 0\n",
    "    while True:\n",
    "        cur_char_type = get_char_type(doc[i])\n",
    "        for j in range(i+1, doclen):\n",
    "            if cur_char_type == 'f': #当前符号为引号，找下一个引号\n",
    "                if get_char_type(doc[j]) == 'f':\n",
    "                    break\n",
    "            elif get_char_type(doc[j]) != cur_char_type: #当前符号非引号，找下一个状态变化\n",
    "                break\n",
    "        if cur_char_type == 's': #对多个空格连续出现的情况进行合并\n",
    "            result.append('s ')\n",
    "        elif cur_char_type == 'f': #对引号只提取引号内字符串\n",
    "            result.append(cur_char_type + doc[i+1:j])\n",
    "            j += 1\n",
    "        else:\n",
    "            result.append(cur_char_type + doc[i:j])\n",
    "        i = j\n",
    "        if i >= doclen - 1:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "parse_query(\"拜仁慕尼黑0-5 AND 德国杯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8d87de5e-6773-464a-b5b0-fe8ecb374c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(self.get_word_match('拜仁') & self.get_word_match('慕尼黑')) & self.get_word_match('0-5') & (self.get_word_match('德国杯'))\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_part(part):\n",
    "    \"\"\"将带有类别标记的解析结果段 转化为 eval能进行计算的代码段\n",
    "\n",
    "    Args:\n",
    "        part:带有类别标记的解析结果段\n",
    "\n",
    "    Results:\n",
    "        eval能进行计算的代码段字符串(调用 term_match() 进行计算)\n",
    "    \"\"\"\n",
    "    flag = part[0]\n",
    "    if flag == 'e':\n",
    "        return \"self.get_word_match('{}')\".format(part[1:])\n",
    "    elif flag == 'c':\n",
    "        return \"(self.get_word_match('{}'))\".format(\n",
    "            \"') & self.get_word_match('\".join(jieba.cut(part[1:])))\n",
    "\n",
    "def query_to_set_expression(query):\n",
    "    query_new_parts = []\n",
    "    all_parts = parse_query(query)\n",
    "    idx = 0\n",
    "    cache = ''\n",
    "    count_parts = len(all_parts)\n",
    "    while idx < count_parts:\n",
    "        if all_parts[idx][1:] == '(' or all_parts[idx][1:] == ')':\n",
    "            query_new_parts.append(all_parts[idx])\n",
    "        elif all_parts[idx][1:] == ' ' or all_parts[idx][1:] == '':\n",
    "            query_new_parts.append(' ')\n",
    "        elif all_parts[idx][1:] in ('and', 'AND', '+'):\n",
    "            query_new_parts.append('&')\n",
    "        elif all_parts[idx][1:] in ('or', 'OR'):\n",
    "            query_new_parts.append('|')\n",
    "        elif all_parts[idx][1:] in ('not', 'NOT', '-'):\n",
    "            query_new_parts.append('-')\n",
    "        elif (idx + 1 < count_parts #对连续的内容分段结果集合中间加”&“运算符\n",
    "              and all_parts[idx+1][1:] not in (' ', ')')): \n",
    "            query_new_parts.append(\"{} & \".format(conv_part(all_parts[idx])))\n",
    "        elif (idx + 2 < count_parts #处理词间、词与符号间空格的情况\n",
    "              and all_parts[idx+1][1:] == \" \" \n",
    "              and all_parts[idx+2][1:] not in (')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ')): \n",
    "            query_new_parts.append(\"{} & \".format(conv_part(all_parts[idx])))\n",
    "            idx += 2\n",
    "            continue\n",
    "        else:\n",
    "            query_new_parts.append(conv_part(cache + all_parts[idx]))\n",
    "            cache = '' #合并完成清空缓存\n",
    "        idx += 1\n",
    "    query_new = ''.join(query_new_parts)\n",
    "    return query_new\n",
    "\n",
    "query_to_set_expression(\"拜仁慕尼黑0-5 AND 德国杯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2d179efc-4a4d-4ca5-bbca-fc8f0fcfbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del conv_part\n",
    "del query_to_set_expression\n",
    "del parse_doc\n",
    "del get_char_type\n",
    "del parse_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0f39f1ea-6dbf-4ac3-aee0-2596319e7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearchC6V1(MySearchC6V0):\n",
    "    \"\"\"\n",
    "    C3V0: Base class for Search Engine.\n",
    "    C3V1: Data multiplication added.\n",
    "    C3V2: Sorting optimization.\n",
    "    C3V3: Add lowered version of docs.\n",
    "    C3V4: For long doc.\n",
    "    C3V5: Caching search results.\n",
    "    C3V6: Pre-caching all words in docs.\n",
    "    C3V7: Add Serialize/UnSerialize.\n",
    "    C4V1: Add basic Bool query support\n",
    "    C4V2: Add wordseg to get_word_match()\n",
    "    ----------------C5V0-----------------\n",
    "    C5V1: Use VSMTFIDF.score() as score\n",
    "    C5V2: Use BM25.score() as score\n",
    "    C5V3: Use MiddleRank -> BM25\n",
    "    ----------------C6V0-----------------\n",
    "    C6V1: Add parse_doc to include whole Eng/Num string\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    filename : str\n",
    "        file name of doc data\n",
    "    multi_factor : int\n",
    "        data multiplication factor(default 1)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    load_data(filename):\n",
    "        load data from file.\n",
    "    save_data(filename):\n",
    "        save data to file\n",
    "    pre_cache_all():\n",
    "        Pre-caching all words in docs.\n",
    "    highlight(text, keyword):\n",
    "        highlight text with keyword.\n",
    "    score(text, keyword):\n",
    "        get score of text for a query.\n",
    "    get_word_match(self, keyword):\n",
    "        get doc set containing keyword.\n",
    "    search(keyword, num=15):\n",
    "        get top num search results of a query.\n",
    "    render(result_list, keyword):\n",
    "        output search results with highlight.\n",
    "    query_to_set_expression(query):\n",
    "        convert bool query to set expression(for eval process).\n",
    "    get_word_match(word):\n",
    "        get match set of the word.\n",
    "    def mid_score(query, tid):\n",
    "        get middle-rank score of doc(tid) according to query\n",
    "    def cosine(vec1, vec2):\n",
    "        get cosine similarity between vec1 and vec2\n",
    "    def dot(vec1, vec2):\n",
    "        get dot product of vec1 and vec2\n",
    "    def parse_doc(doc):\n",
    "        parse doc into terms, including whole Eng/Num string\n",
    "    def get_char_type(c):\n",
    "        get type of character c\n",
    "    def parse_query(q):\n",
    "        parse query q to Cn/En/Num parts with prefix\n",
    "    def conv_part(part):\n",
    "        convert part to set call\n",
    "    \"\"\"\n",
    "    \n",
    "    def parse_doc(self, doc):\n",
    "        result = []\n",
    "        state_last = ''\n",
    "        cache = ''\n",
    "        for c in doc:\n",
    "            state_c = c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.', '：', '/')\n",
    "            if c == ' ':\n",
    "                if state_last:\n",
    "                    result.append(cache)\n",
    "                else:\n",
    "                    result.extend(list(jieba.cut_for_search(cache)))\n",
    "                result.append(' ')\n",
    "                cache = ''\n",
    "                state_last = '' \n",
    "            else:\n",
    "                if state_c == state_last:\n",
    "                    cache += c\n",
    "                else:\n",
    "                    if state_last != '':\n",
    "                        if state_last:\n",
    "                            result.append(cache)\n",
    "                        else:\n",
    "                            result.extend(list(jieba.cut_for_search(cache)))\n",
    "                    cache = c\n",
    "                state_last = state_c\n",
    "        if cache:\n",
    "            if state_last:\n",
    "                result.append(cache)\n",
    "            else:\n",
    "                result.extend(list(jieba.cut_for_search(cache)))\n",
    "        return result\n",
    "    \n",
    "    def pre_cache_all(self):\n",
    "        for tid, doc in enumerate(self.docs_lower):\n",
    "            doc_tf_dict = defaultdict(int)\n",
    "            doc_word_count = 0\n",
    "            for word in self.parse_doc(doc):\n",
    "                self.search_cache[word].add(tid)\n",
    "                doc_tf_dict[word] += 1\n",
    "                doc_word_count += 1\n",
    "            for word in doc_tf_dict:\n",
    "                doc_tf_dict[word] /= doc_word_count\n",
    "            self.doc_word_dict.append(doc_tf_dict)\n",
    "            \n",
    "    def get_char_type(self, c):\n",
    "        \"\"\"返回当前字符的类型(e,c,s,f,b)\n",
    "\n",
    "        Args:\n",
    "            c:要进行判断的单个字符\n",
    "\n",
    "        Results:\n",
    "            返回判断结果(前缀)：e为英文，c为中文，s为空格，f为引号，b为括号\n",
    "        \"\"\"\n",
    "        result = 'c'\n",
    "        if c in string.ascii_letters \\\n",
    "                or c.isdigit() \\\n",
    "                or c in ('-', ':', '.', '：', '/'):\n",
    "            result = 'e'\n",
    "        elif c == '\"':\n",
    "            result = 'f'\n",
    "        elif c == ' ':\n",
    "            result = 's'\n",
    "        elif c in ('(', ')'):\n",
    "            result = 'b'\n",
    "        return result\n",
    "\n",
    "    def parse_query(self, doc):\n",
    "        \"\"\"对查询进行自定义解析，保留英文串，对中文串原型插入\n",
    "\n",
    "        Args:\n",
    "            doc:待解析的原始文档\n",
    "\n",
    "        Returns:\n",
    "            解析结果列表，元素是带有串类型标记(首字符，e为英文，c为中文，s为空格，f为引号，b为括号)\n",
    "            的切分term结果\n",
    "        \"\"\"\n",
    "        doc = doc.lower() + ' ' #解决末位字符状态切换问题的小技巧\n",
    "        result = []\n",
    "        doclen = len(doc)\n",
    "        i = 0\n",
    "        while True:\n",
    "            cur_char_type = self.get_char_type(doc[i])\n",
    "            for j in range(i+1, doclen):\n",
    "                if cur_char_type == 'f': #当前符号为引号，找下一个引号\n",
    "                    if self.get_char_type(doc[j]) == 'f':\n",
    "                        break\n",
    "                elif self.get_char_type(doc[j]) != cur_char_type: #当前符号非引号，找下一个状态变化\n",
    "                    break\n",
    "            if cur_char_type == 's': #对多个空格连续出现的情况进行合并\n",
    "                result.append('s ')\n",
    "            elif cur_char_type == 'f': #对引号只提取引号内字符串\n",
    "                result.append(cur_char_type + doc[i+1:j])\n",
    "                j += 1\n",
    "            else:\n",
    "                result.append(cur_char_type + doc[i:j])\n",
    "            i = j\n",
    "            if i >= doclen - 1:\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    def conv_part(self, part):\n",
    "        \"\"\"将带有类别标记的解析结果段 转化为 eval能进行计算的代码段\n",
    "\n",
    "        Args:\n",
    "            part:带有类别标记的解析结果段\n",
    "\n",
    "        Results:\n",
    "            eval能进行计算的代码段字符串(调用 term_match() 进行计算)\n",
    "        \"\"\"\n",
    "        flag = part[0]\n",
    "        if flag == 'e':\n",
    "            return \"self.get_term_match('{}')\".format(part[1:])\n",
    "        elif flag == 'c':\n",
    "            return \"(self.get_term_match('{}'))\".format(\n",
    "                \"') & self.get_term_match('\".join(jieba.cut(part[1:])))\n",
    "\n",
    "    def query_to_set_expression(self, query):\n",
    "        query_new_parts = []\n",
    "        all_parts = self.parse_query(query)\n",
    "        idx = 0\n",
    "        cache = ''\n",
    "        count_parts = len(all_parts)\n",
    "        while idx < count_parts:\n",
    "            if all_parts[idx][1:] == '(' or all_parts[idx][1:] == ')':\n",
    "                query_new_parts.append(all_parts[idx])\n",
    "            elif all_parts[idx][1:] == ' ' or all_parts[idx][1:] == '':\n",
    "                query_new_parts.append(' ')\n",
    "            elif all_parts[idx][1:] in ('and', 'AND', '+'):\n",
    "                query_new_parts.append('&')\n",
    "            elif all_parts[idx][1:] in ('or', 'OR'):\n",
    "                query_new_parts.append('|')\n",
    "            elif all_parts[idx][1:] in ('not', 'NOT', '-'):\n",
    "                query_new_parts.append('-')\n",
    "            elif (idx + 1 < count_parts #对连续的内容分段结果集合中间加”&“运算符\n",
    "                  and all_parts[idx+1][1:] not in (' ', ')')): \n",
    "                query_new_parts.append(\"{} & \".format(self.conv_part(all_parts[idx])))\n",
    "            elif (idx + 2 < count_parts #处理词间、词与符号间空格的情况\n",
    "                  and all_parts[idx+1][1:] == \" \" \n",
    "                  and all_parts[idx+2][1:] not in (')', 'and', 'AND', '+', 'or', 'OR', 'NOT', 'not', '+', '-', ' ')): \n",
    "                query_new_parts.append(\"{} & \".format(self.conv_part(all_parts[idx])))\n",
    "                idx += 2\n",
    "                continue\n",
    "            else:\n",
    "                query_new_parts.append(self.conv_part(cache + all_parts[idx]))\n",
    "                cache = '' #合并完成清空缓存\n",
    "            idx += 1\n",
    "        query_new = ''.join(query_new_parts)\n",
    "        return query_new\n",
    "    \n",
    "    def get_term_match(self, term):\n",
    "        return self.search_cache.get(term, set()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7ed3fbee-edb5-4ba5-a9d8-11d3fe701b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = MySearchC6V1('titles_l.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "af81b569-30c2-4654-99c2-d9a2fc5c6e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1、德国杯-爆冷!拜仁0-5惨遭门兴血洗 后防灾难级表现<br/>北京时间10月28日凌晨2：45，2021-2022赛季德国杯第二轮继续进行，拜仁慕尼黑客场对阵门兴格拉德巴赫，上半场，科内闪电破门，本塞拜尼连入两球，拜仁0-3落后。下半场，恩博洛梅开二度。最终，拜仁客场0-5惨败于门兴，德国杯惨遭淘汰......"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = '拜仁0-5 AND 德国杯'\n",
    "search_result = searcher.search(query, num=10)\n",
    "searcher.render(search_result, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c5cb3869-90bf-4c3d-926f-2decee979a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(self.get_term_match('拜仁')) & self.get_term_match('0-5') & (self.get_term_match('德国杯'))\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.query_to_set_expression(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcaec3-d42b-4b6e-8d5f-348ff669818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'to be or not to be'\n",
    "search_result = searcher.search(query, num=10)\n",
    "searcher.render(search_result, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9adb5-ebbf-4be2-a994-cab99ef46e52",
   "metadata": {},
   "source": [
    "### 对特殊短语的查询需求，比如“0-3”，“to be or not to be” —— 临近查询 \n",
    "#### - 采用特殊索引结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "091686fa-59ed-42a2-837f-16c999e7c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearchC6V2(MySearchC6V1):\n",
    "    \"\"\"\n",
    "    C3V0: Base class for Search Engine.\n",
    "    C3V1: Data multiplication added.\n",
    "    C3V2: Sorting optimization.\n",
    "    C3V3: Add lowered version of docs.\n",
    "    C3V4: For long doc.\n",
    "    C3V5: Caching search results.\n",
    "    C3V6: Pre-caching all words in docs.\n",
    "    C3V7: Add Serialize/UnSerialize.\n",
    "    C4V1: Add basic Bool query support\n",
    "    C4V2: Add wordseg to get_word_match()\n",
    "    ----------------C5V0-----------------\n",
    "    C5V1: Use VSMTFIDF.score() as score\n",
    "    C5V2: Use BM25.score() as score\n",
    "    C5V3: Use MiddleRank -> BM25\n",
    "    ----------------C6V0-----------------\n",
    "    C6V1: Add parse_doc to include whole Eng/Num string\n",
    "    C6V2: Add 2gram-Inverted-Index and get_frag_match()\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    filename : str\n",
    "        file name of doc data\n",
    "    multi_factor : int\n",
    "        data multiplication factor(default 1)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    load_data(filename):\n",
    "        load data from file.\n",
    "    save_data(filename):\n",
    "        save data to file\n",
    "    pre_cache_all():\n",
    "        Pre-caching all words in docs.\n",
    "    highlight(text, keyword):\n",
    "        highlight text with keyword.\n",
    "    score(text, keyword):\n",
    "        get score of text for a query.\n",
    "    get_word_match(self, keyword):\n",
    "        get doc set containing keyword.\n",
    "    search(keyword, num=15):\n",
    "        get top num search results of a query.\n",
    "    render(result_list, keyword):\n",
    "        output search results with highlight.\n",
    "    query_to_set_expression(query):\n",
    "        convert bool query to set expression(for eval process).\n",
    "    get_word_match(word):\n",
    "        get match set of the word.\n",
    "    def mid_score(query, tid):\n",
    "        get middle-rank score of doc(tid) according to query\n",
    "    def cosine(vec1, vec2):\n",
    "        get cosine similarity between vec1 and vec2\n",
    "    def dot(vec1, vec2):\n",
    "        get dot product of vec1 and vec2\n",
    "    def parse_doc(doc):\n",
    "        parse doc into terms, including whole Eng/Num string\n",
    "    def get_char_type(c):\n",
    "        get type of character c\n",
    "    def parse_query(q):\n",
    "        parse query q to Cn/En/Num parts with prefix\n",
    "    def conv_part(part):\n",
    "        convert part to set call\n",
    "    def get_frag_match(frag):\n",
    "        get docs matching frag\n",
    "    def get_term_match(term):\n",
    "        get docs macthing term\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, multi_factor=1):\n",
    "        self.docs = []\n",
    "        self.docs_lower = []\n",
    "        self.doc_word_dict = [] #记录文档-词关系\n",
    "        self.search_cache = defaultdict(set)\n",
    "        self.search_cache_b = defaultdict(set) #2gram索引\n",
    "        self.multi_factor = multi_factor\n",
    "        self.load_data(filename)\n",
    "    \n",
    "    def pre_cache_all(self):\n",
    "        for tid, doc in enumerate(self.docs_lower):\n",
    "            doc_tf_dict = defaultdict(int)\n",
    "            doc_word_count = 0\n",
    "            for word in self.parse_doc(doc):\n",
    "                self.search_cache[word].add(tid)\n",
    "                doc_tf_dict[word] += 1\n",
    "                doc_word_count += 1\n",
    "            for word in doc_tf_dict:\n",
    "                doc_tf_dict[word] /= doc_word_count\n",
    "            self.doc_word_dict.append(doc_tf_dict)\n",
    "            \n",
    "            doclen = len(doc)\n",
    "            for i in range(doclen-1):\n",
    "                term = doc[i:i+2]\n",
    "                self.search_cache_b[term].add(tid)\n",
    "\n",
    "    def get_frag_match(self, frag):\n",
    "        \"\"\"对片段frag用ngram索引实现原样搜索\n",
    "        \n",
    "        Args:\n",
    "            frag:要原样搜索的字符串\n",
    "            \n",
    "        Results:\n",
    "            片段原样搜索的结果(文档ID)集合\n",
    "        \"\"\"\n",
    "        frag = frag.lower() #大小写归一化\n",
    "        result = None\n",
    "        doclen = len(frag)\n",
    "        for i in range(doclen - 1):\n",
    "            term = frag[i:i+2]\n",
    "            if result is None:\n",
    "                result = self.search_cache_b.get(term, set())\n",
    "            else:\n",
    "                result = result & self.search_cache_b.get(term, set())\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0aac92bd-b497-4514-ba3e-43fa7d49c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = MySearchC6V2('titles_l.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9a2419c4-63ad-4d12-a833-54450bf5919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1301}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.get_frag_match('to be or not to be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94d832-582f-453f-8450-9afc0f6c1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.words.words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e5244-8aba-4f1f-aca2-b23151e21b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "class Corrector():\n",
    "    \"\"\"用二元索引实现拼写校正\n",
    "    \n",
    "    Attributes:\n",
    "        index_b: 检索使用的二元索引\n",
    "        max_id: 当前索引的单词最大ID\n",
    "        doc_list: 索引单词原文\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        \"\"\"初始化，用NLTK的words词典构建倒排索引\n",
    "        \"\"\"\n",
    "        self.index_b = dict() #ngram索引\n",
    "        self.max_id = 0\n",
    "        self.doc_list = [] \n",
    "        \n",
    "        for doc in nltk.corpus.words.words():\n",
    "            self.add_doc(doc)\n",
    "            \n",
    "    def add_doc(self, doc):\n",
    "        \"\"\"向索引中添加新词(单词的二元索引)\n",
    "        \n",
    "        Args:\n",
    "            doc:待检索的单词\n",
    "        \n",
    "        Returns:\n",
    "            新增单词ID\n",
    "        \"\"\"\n",
    "        self.doc_list.append(doc)\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        #构建二元索引\n",
    "        doclen = len(doc)\n",
    "        for i in range(doclen-1):\n",
    "            term = doc[i:i+2]\n",
    "            if term in self.index_b: \n",
    "                self.index_b[term].append(self.max_id)\n",
    "            else:\n",
    "                self.index_b[term] = [self.max_id]\n",
    "                \n",
    "        self.max_id += 1\n",
    "        return self.max_id - 1\n",
    "    \n",
    "    def correct(self, word, limit=5):\n",
    "        \"\"\"拼写校正函数\n",
    "        \n",
    "        Args:\n",
    "            word:待校正的词\n",
    "            limit:返回结果的最大条数，默认值为5\n",
    "            \n",
    "        Returns:\n",
    "            最可能的校正单词列表\n",
    "        \"\"\"\n",
    "        word = word.lower() #大小写归一化\n",
    "        result = []\n",
    "        docid_list = []\n",
    "        doclen = len(word)\n",
    "        for i in range(doclen - 1):\n",
    "            term = word[i:i+2]\n",
    "            docid_list += self.index_b.get(term, [])\n",
    "        docid_counter = Counter(docid_list)\n",
    "        count = 0\n",
    "        for elem in docid_counter.most_common(300):\n",
    "            cor_word = self.doc_list[elem[0]]\n",
    "            if len(cor_word) >= doclen - 1 and len(cor_word) <= doclen + 1:\n",
    "                result.append(cor_word)\n",
    "                count += 1\n",
    "                if count > limit:\n",
    "                    break\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf7e0f-387b-4770-963d-659db1add17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = Corrector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c31cd-5d3f-4160-a85e-bac10031bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cor.correct('retrival'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2833f-0d72-48dc-b88b-05c6f1540ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '华为Mate30采用安卓系统'\n",
    "\n",
    "n = 6\n",
    "word_set = set(\n",
    "    ['华为' ,'安卓', '安卓系统'])\n",
    "\n",
    "#正向最大分词\n",
    "i = 0\n",
    "result_f = []\n",
    "while True:\n",
    "    end_idx = i + n\n",
    "    if end_idx > len(doc):\n",
    "        end_idx = len(doc)\n",
    "    for j in range(end_idx, i, -1):\n",
    "        if doc[i:j] in word_set:\n",
    "            break\n",
    "    result_f.append(doc[i:j])\n",
    "    i = j\n",
    "    if i == len(doc):\n",
    "        break\n",
    "print('|'.join(result_f))\n",
    "\n",
    "#逆向最大分词\n",
    "i = len(doc)\n",
    "result_b = []\n",
    "while True:\n",
    "    end_idx = i - n\n",
    "    if end_idx < 0:\n",
    "        end_idx = 0\n",
    "    for j in range(end_idx, i):\n",
    "#         print(j,i,doc[j:i])\n",
    "        if doc[j:i] in word_set:\n",
    "            break\n",
    "    result_b.insert(0, doc[j:i])\n",
    "    i = j\n",
    "    if i == 0:\n",
    "        break\n",
    "print('|'.join(result_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41d400-bb9b-4b42-93f3-5b27ea1a6dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
